# Test CNS On OCP with Apps

The target of the test is to see the performance of CNS under the IO workload
 generated by the applications shipped with OCP templates. In this blog, we 
 focus on Jenkins, Redis, JBossAMQ and GitWorkload. The code of the test 
 can be found in [svt repo](https://github.com/openshift/svt/tree/master/storage).
 
## OCP cluster

OCP version: 3.11.0-0.25.0.git.0.a95c194.el7

CNS image version:
* rhgs-gluster-block-prov-rhel7:3.4.0-X
* rhgs-server-rhel7:3.4.0-X
* rhgs-volmanager-rhel7:3.4.0-X

| role    | no.  | type          |
|---------|------|---------------|
| master  | 3    | 4xlarge       |
| infra   | 3    | 10xlarge      |
| compute | 250+ | large/2xlarge |
| cns     | 3    | 4xlarge       |

## Generate IO workload

For each application, we choose or develop a tool for benchmarking and 
repeatedly call the tool to generate the IO workload for the application. 
We containerize the tool if the too itself is a standalone 
application and it requires its own running environment.

For example, we use [YCSB](https://github.com/brianfrankcooper/YCSB/wiki) to test
against Redis server which requires Java/Maven. Then the YCSB container runs in a
separate pod as Redis pod. Those two pods stays in a test project.

A PVC is used in the application pod to store the data generated by the application
via the benchmark tool. We create such test projects as many as possible. All the
PVCs are backed up by CNS, providing PVC type *gluster-file* and *gluster-block*.
In this way of scaling out, we monitor how CNS performs to support the
applications under consideration. We stop increasing the number of test projects
either when there is no space left to create PVCs or we oversee some performance
issues.

 
## Results

For all test project, the benchmark tool is called by 10 iterations. We will get
a performance measurement generated by the tool for each iteration. Jenkins test
counts *total* failed builds for all projects and all iterations while the rest
calculates the *average* value of the all outputs generated by the respective tool. 

### Jenkins

Pod parameters: memory limit: 6G; PVC size: 3G; PVC type: gluster-block.

| instance | total build | failed build | fail rate (%) |
|----------|-------------|--------------|---------------|
| 1        | 30          | 1            | 03.3          |
| 10       | 300         | 60           | 20.0          |
| 50       | 1500        | 304          | 20.3          |
| 100      | 3000        | 589          | 19.6          |
| 200      | 6000        | 1017         | 17.0          |

On each Jenkins instance, we trigger via Jenkins rest api 3 builds concurrently.
Then we check the status of the builds and count the failed builds.

The Jenkins job definitions land on Jenkins instance by [JJB](https://docs.openstack.org/infra/jenkins-job-builder/).
Each of them is Jenkins multi-branch pipeline job which checks out [a springboot
application repo](https://github.com/hongkailiu/gs-spring-boot) and run Maven
tasks `install` and then achieve the built jar file and junit test results.

The failed builds complains about incomplete/empty jar files as Maven dependencies.
We believe that cleaning up the Maven repo folder after each iteration should
increase the successful build rate while we leave this as future work. 

The test is not using gluster-file because of [1588562](https://bugzilla.redhat.com/show_bug.cgi?id=1588562).

### Redis

Pod parameters: memory limit: 6G; PVC size: 3G.

gluster-file: Did not go further because of no space left on CNS.

| instance\throughput | load      | run       |
|---------------------|-----------|-----------|
| 1                   | 13,532.56 | 25,577.91 |
| 10                  | 13,969.80 | 26,771.05 |
| 100                 | 12,113.42 | 24,425.35 |
| 250                 | 10,150.21 | 20,870.68 |


gluster-block: Did not go further because of [1627962](https://bugzilla.redhat.com/show_bug.cgi?id=1627962).

| instance\throughput | load      | run       |                       |
|---------------------|-----------|-----------|-----------------------|
| 1                   | 14,191.68 | 27,644.39 |                       |
| 10                  | 14,134.37 | 27,721.02 | missing 0;2 results   |
| 50                  | 13,182.41 | 26,937.44 | missing 30;16 results |

We use YCSB's [workload_template](https://github.com/brianfrankcooper/YCSB/blob/master/workloads/workload_template)
to load and run the data, and generate `overall throughput (ops/sec)`. 

_missing results_ indicates that exception occurred during the test.

### JBossAMQ

Pod parameters: memory limit: not set in the OCP template; PVC size: 1G; PVC type: gluster-file.

| project | throughput |
|---------|------------|
| 1       | 87,972.24  |
| 10      | 83,035.75  |
| 50      | 20,220.86  |
| 100     | 9,790.12   |
| 250     | 3,799.39   |

[activemq-performance-module](http://activemq.apache.org/activemq-performance-module-users-manual.html)
is chosen for the test. It starts producer and consumer of a topic for 5 minutes
and we take the `System Total Throughput` value from its output.

The test is not using gluster-file the AMQ template requires the PVC supports the
access mode `readWriteMany`.

### GitWorkload

Pod parameters: memory limit: not set; PVC size: 3G; PVC type: gluster-block.

| project | time   |
|---------|--------|
| 1       | 23.90  |
| 10      | 23.94  |
| 50      | 46.40  |
| 100     | 60.71  |
| 150     | 117.75 |

GitWorkload is not an application from OCP template but it is somehow a 
 routine workload in [openshift.io](openshift.io). It follows the same 
 testing method as other applications in this blog.
 
We use [this script](https://github.com/openshift/svt/blob/master/storage/git/files/scripts/test-git.sh)
to generate the IO workload. It clones the [Eclipse Che repo](https://github.com/eclipse/che.git)
(currently about 200M ) and then does `tar/untar/rm` on the repo folder and output
the time spent on those steps.

The test is not using gluster-file because of [1589359](https://bugzilla.redhat.com/show_bug.cgi?id=1589359).

### System load on CNS node

The data are collected from the CNS nodes by [pbench](https://github.com/distributed-system-analysis/pbench). The following shows for 1 CNS node.

|              | Jenkins + block | Redis + file | Redis + block | AMQ + file | Git + block |
|--------------|-----------------|--------------|---------------|------------|-------------|
| project      | 200             | 250          | 50            | 250        | 150         |
| iostat       | 5000            | 2500         | 4500          | 9000       | 11000       |
| sar: cpu     | 600             | 200          | 120           | 800        | 800         |
| sar: mem     | 5               | 61           | <5            | 60         | 56          |
| sar: network | 1200            | 1800         | 1400          | 200        | 3500        |

## Issues and workaround

* PVC provisioning and deleting: First of all, it has to be admitted that PVC
    provisioning and deleting on OCP 3.11 are much better than OCP 3.10 after
    improvement in [1600160](https://bugzilla.redhat.com/show_bug.cgi?id=1600160)
    and [1573304](https://bugzilla.redhat.com/show_bug.cgi?id=1573304). Otherwise,
    it would be even harder to achieve the number of projects above.
    
    The test create all test projects with Ansible-playbooks. 
    In theory, we should see all pods are running and ready.
    However, we observe that many deploy pods are in error state. The reason
    is that k8s monitors the Application pods via a deploy pod only up to 600
    seconds. If the Application pods are not running for any reason, the deploy
    pod gives up and become `error`.

    ```sh
    # oc logs -n storage-test-jenkins-160            jenkins-1-deploy
    --> Scaling jenkins-1 to 1
    error: update acceptor rejected jenkins-1: pods for rc 'storage-test-jenkins-160/jenkins-1' took longer than 600 seconds to become available
    ```

    It is what happened in the test if CNS cannot create PVCs quickly enough.
    The workaround is to give a pause (10 to 30 seconds) after each project 
    so that less concurrent requests of PVC in CNS.
    
    We also observe that the number of bound *glustr-block* PVCs stops increasing
    after some time.
    This is because we hit [1609360](https://bugzilla.redhat.com/show_bug.cgi?id=1609360).
    Scaling down/up the block-provisioner dc helps in this case.
    We need to act this quickly enough before the deploy pods run into error.
    
    When cleaning up the test projects, we need give the pause too:
    
    ```bash
    $ for i in {1..200}; do oc delete project "storage-test-jenkins-$i" --wait=false; sleep 30; done
    ``` 
     
    However, if deleting still stuck, then this is the rescue
    
    * reboot the CNS node, or
    * un/re-install CNS if reboot does not help
    
    The test itself for each application runs for about 2 hours while creating/deleting
    the test project sometimes can take much than that. *The above bugs are
    definitely blockers for the test going beyond the present number of projects
    and on the way of test automation.*
    As a result, we reuse the existing projects and only create the new ones in
    an incremental way.

* block hosting volumes (BHV): Currently, the BHV will not be released even if
    all the blocks on it are released. We have to use heketi cli to delete it
    before [1625304](https://bugzilla.redhat.com/show_bug.cgi?id=1625304) gets
    implemented. However, we sometimes see `target is busy`. In this case, wait
    for 2 minutes and try again.

* CNS device volume size and `openshift_storage_glusterfs_block_host_vol_size`:
    In our test, we have a 894G NVMe device for each CNS node. At the installation
    of CNS, `openshift_storage_glusterfs_block_host_vol_size=350` was used at the
    beginning which is decreased to `200` for better use of space on the disk.

* Jenkins image tag and route for OCP: The test depends on `imageStreamTag` in 
    `openshift` namespace.
    For example, `jenkins:2` tag has to be available in order to run the Jenkins
    test. If it refers to `latest` tag, edit the `imagestream` to use the `v3.10`
    tag because the `latest` tag points to `v3.6` [1628611](https://bugzilla.redhat.com/show_bug.cgi?id=1628611)
    intentionally.
    
    ```bash
    $ oc edit -n openshift jenkins
    ```
    
    The application route must work for Jenkins test because it is used in Jenkins
    test for rest api calls.