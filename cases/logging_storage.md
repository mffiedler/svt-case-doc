# Storage Test for Logging Stack

## Test env.

### IO1
Follow [Case OCP-15841](https://polarion.engineering.redhat.com/polarion/#/project/OSE/workitem?id=OCP-15841).

1 master, 1 infra and 2 computes. All m4.xlarge instances.

### Glusterfs

1 master, 1 infra and 5 computes. All m4.xlarge instances.

## Configure default storage class

### IO1

```sh
# oc create -f https://raw.githubusercontent.com/hongkailiu/svt-case-doc/master/files/sc_io1.yaml
# oc get sc
NAME            PROVISIONER             AGE
gp2             kubernetes.io/aws-ebs   23h
io1 (default)   kubernetes.io/aws-ebs   22h

```

### Glusterfs

#### Install CNS

```
glusterfs_devices=["/dev/xvdf"]
openshift_storage_glusterfs_wipe=true
openshift_storage_glusterfs_image=registry.access.redhat.com/rhgs3/rhgs-server-rhel7
openshift_storage_glusterfs_version=3.3.0-362
openshift_storage_glusterfs_heketi_image=registry.access.redhat.com/rhgs3/rhgs-volmanager-rhel7
openshift_storage_glusterfs_heketi_version=3.3.0-364
openshift_hosted_registry_glusterfs_swap=true
openshift_storage_glusterfs_block_deploy=true
openshift_storage_glusterfs_block_image=registry.access.redhat.com/rhgs3/rhgs-gluster-block-prov-rhel7
openshift_storage_glusterfs_block_version=3.3.0-362
openshift_storage_glusterfs_block_host_vol_size=800
openshift_storage_glusterfs_block_storageclass=true
```

## Check docker configuration

```sh
### Check docker configuration file on each node
# cat /etc/sysconfig/docker
...
OPTIONS='--selinux-enabled --log-opt max-size=10M --log-opt max-file=3 --signature-verification=false'
...

### Restart docker and atomic-openshift-node services if we need to modify the above file
# systemctl restart docker.service atomic-openshift-node.service
```


## Deploy logging stack

Set up buffer size limit for fluentd:

```
openshift_logging_fluentd_buffer_size_limit=16m
#in case of glusterfs
openshift_logging_es_pvc_size=800Gi
###The following line has be to commented out
#openshift_logging_storage_kind=dynamic
openshift_logging_es_pvc_dynamic=false
###use glusterblock; glusterfs-storage does not even work for installation of logging stack
openshift_logging_elasticsearch_pvc_storage_class_name=glusterblock

```

After running logging playbook, make sure only primary nodes are labeled for fluentd:

```sh
oc label node --all logging-infra-fluentd-
oc label node -l region=primary logging-infra-fluentd=true
```

Move es pod to infra:

```sh
# oc edit dc logging-es-data-master-xxxxx
     dnsPolicy: ClusterFirst
     nodeSelector:
        region: infra
        zone: default

```

Set the elasticsearch threadpool bulk queue size to 200 (Optional):

```sh
# POD=logging-es-data-master-hye5503q-1-g4w27
# oc exec -n logging $POD -- curl --connect-timeout 2 -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key -XPUT https://localhost:9200/_cluster/settings -d '{"persistent" : {"threadpool.bulk.queue_size" : 200}}'
# oc exec $POD -- curl -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key https://localhost:9200/_cluster/settings | python -mjson.tool
```

## Generate logs

```sh
# cd svt/openshift_scalability
# curl -L -o config/cl_logtest_rate500mpsec_30min.yaml https://raw.githubusercontent.com/hongkailiu/svt-case-doc/master/files/cl_logtest_rate500mpsec_30min.yaml
# ./cluster-loader.py -vf  config/cl_logtest_rate500mpsec_30min.yaml

### Monitor if all logs are generated by the pod
root@ip-172-31-1-162: ~/svt/openshift_scalability # oc get pod -n lograte-500mpsec-0
NAME                   READY     STATUS    RESTARTS   AGE
centos-logtest-9mgpq   1/1       Running   0          1m

# oc logs --tail=1 -n lograte-500mpsec-0   centos-logtest-9mgpq
### OR,
# oc logs -n lograte-500mpsec-0   centos-logtest-9mgpq | tail -1
2018-01-03 16:17:13,967 - SVTLogger - INFO - centos-logtest-9mgpq : 30000 : ...
```

Compare the result in es:

```sh
### rsh to es pod
sh-4.2$ curl --connect-timeout 2 -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key https://logging-es:9200/_cat/indices?v | grep logr

### for debugging
sh-4.2$ curl -s -k --cert /etc/elasticsearch/secret/admin-cert   --key /etc/elasticsearch/secret/admin-key https://localhost:9200/_cat/thread_pool?v\&h=host,bulk.completed,bulk.rejected,bulk.queue,bulk.active,bulk.queueSize

sh-4.2$ curl --connect-timeout 2 -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key https://logging-es:9200/_cat/indices?v | grep aaa001 | awk '{print $6}' | awk '{s+=$1} END {print s}'
```

## Result

* 20170112: oc(3.7.9-1.git.0.7c71a2d.el7), logging(v3.7.22-1), gluster(3.3.0-362 with heketi 3.3.0-364):10 projoects X 2 nodes: block-volume
     * "--line-length 1024 --word-length 7 --rate 4500 --time 0 --fixed-line --num-lines 1080000\n" (750 rate/node X 4 hours): no loss (21600000)
     * "--line-length 1024 --word-length 7 --rate 6000 --time 0 --fixed-line --num-lines 1440000\n" (1000 rate/node X 4 hours): no loss (28800000)

* 20170115: oc(3.9.0-0.19.0.git.0.4b1410e.el7), logging(v3.9.0-0.9.0.0), gluster(3.3.0-362 with heketi 3.3.0-364):10 projoects X 2 nodes: block-volume
     * 750 rate/node X 4 hours: 91% (19586209 / 21600000)
     * 1000 rate/node X 4 hours: 64% (18442784 / 28800000)

* 20170116: oc(3.9.0-0.19.0.git.0.4b1410e.el7), logging(v3.9.0-0.9.0.0), gp2:10 projoects X 2 nodes:
     * 750 rate/node X 4 hours: 87% (18698762 / 21600000)
     * 1000 rate/node X 4 hours: 67% (19244989 / 28800000)

     

One pod for the logging test tool:

```sh
--line-length 1024 --word-length 7 --rate <rate> --time 0 --fixed-line --num-lines <num_lines>
```

| date        | oc                            | logging images | sha of fluentd | sc                    | rate (logs/min) | logs generated |        logs in es |
|-------------|-------------------------------|----------------|----------------|-----------------------|----------------:|---------------:|------------------:|
| 20170103    | 3.9.0-0.9.0.git.0.f5a6e1d.el7 | v3.9           |                | io1                   |            9000 |          10000 |             10000 |
|             |                               |                |                |                       |           12000 |          10000 |              1451 |
|             |                               |                |                |                       |           15000 |           8000 |              8000 |
|             |                               |                |                |                       |           15000 |          10000 |              1451 |
|             |                               |                |                |                       |           30000 |           8000 |              8000 |
|             |                               |                |                |                       |           30000 |          10000 |              1451 |
|             |                               |                |                |                       |            9000 |         500000 |            500000 |
| 20170104    | 3.9.0-0.9.0.git.0.f5a6e1d.el7 | v3.9.0-0.9.0.0 | 2d4381a3f      | gp2                   |           30000 |         900000 |            900000 |
|             |                               |                |                |                       |           30000 |         900000 |            874368 |
|             |                               |                |                |                       |           30000 |         900000 |            891451 |
|             |                               |                |                |                       |           45000 |        1350000 |           1204946 |
|             |                               |                |                |                       |           45000 |        1350000 |           1179332 |
| 20170109 ^1 | 3.7.9-1.git.0.7c71a2d.el7     | v3.7.22-1      | 9cf349ffb      | gp2                   |           45000 |        1350000 |           1350000 |
|             |                               |                |                |                       |           60000 |        1800000 |           1714740 |
|             |                               |                |                |                       |           60000 |        1800000 |           1714667 |
|             |                               |                |                | glusterfs (3.3.0-362) |           30000 |         900000 |            900000 |
|             |                               |                |                |                       |           45000 |        1350000 |           1350000 |
|             |                               |                |                |                       |           60000 |        1800000 | 1800000 (amazing) |
|             |                               |                |                |                       |           60000 |        1800000 |        1206590 ^2 |
|             |                               |                |                |                       |           60000 |        1800000 |           1800000 |


* ^1: From Jan 9 2017, max-size=100M instead of 10.
* ^2: Health of operations index is RED.

More [results](https://docs.google.com/document/d/1JB8GVYHrPK4TPMQnwViZNA-fdFMpYw-Upkpsa_YL2es/edit?usp=sharing) on logging rate.

```sh
### On the compute node where pod logs are being generated
# pidstat -C fluentd 30
```
* rate 30000: CPU% 50
* 45000: 99 (at the beginning), 74
* 60000: 98+
