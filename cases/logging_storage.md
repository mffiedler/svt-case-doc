# Storage Test for Logging Stack

## Test env.

### IO1
Follow [Case OCP-15841](https://polarion.engineering.redhat.com/polarion/#/project/OSE/workitem?id=OCP-15841).

1 master, 1 infra and 2 computes.

### Glusterfs

## Configure default storage class

### IO1

```sh
# oc create -f https://raw.githubusercontent.com/hongkailiu/svt-case-doc/master/files/sc_io1.yaml
# oc get sc
NAME            PROVISIONER             AGE
gp2             kubernetes.io/aws-ebs   23h
io1 (default)   kubernetes.io/aws-ebs   22h

```

### Glusterfs

#### Install CNS

## Check docker configuration

```sh
### Check docker configuration file on each node
# cat /etc/sysconfig/docker
...
OPTIONS='--selinux-enabled --log-opt max-size=10M --log-opt max-file=3 --signature-verification=false'
...

### Restart docker and atomic-openshift-node services if we need to modify the above file
```


## Deploy logging stack

Set up buffer size limit for fluentd:

```
openshift_logging_fluentd_buffer_size_limit=16m
```

After running logging playbook, make sure only primary nodes are labeled for fluentd:

```sh
oc label node --all logging-infra-fluentd-
oc label node -l region=primary logging-infra-fluentd=true
```

Move es pod to infra:

```sh
# oc edit dc logging-es-data-master-xxxxx
     dnsPolicy: ClusterFirst
     nodeSelector:
        region: infra
        zone: default

```

Set the elasticsearch threadpool bulk queue size to 200:

```sh
# POD=logging-es-data-master-hye5503q-1-g4w27
# oc exec -n logging $POD -- curl --connect-timeout 2 -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key -XPUT https://localhost:9200/_cluster/settings -d '{"persistent" : {"threadpool.bulk.queue_size" : 200}}'
# oc exec $POD -- curl -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key https://localhost:9200/_cluster/settings | python -mjson.tool
```

## Generate logs

```sh
# cd svt/openshift_scalability
# curl -L -o config/cl_logtest_rate500mpsec_30min.yaml https://raw.githubusercontent.com/hongkailiu/svt-case-doc/master/files/cl_logtest_rate500mpsec_30min.yaml
# ./cluster-loader.py -vf  config/cl_logtest_rate500mpsec_30min.yaml

### Monitor if all logs are generated by the pod
root@ip-172-31-1-162: ~/svt/openshift_scalability # oc get pod -n lograte-500mpsec-0
NAME                   READY     STATUS    RESTARTS   AGE
centos-logtest-9mgpq   1/1       Running   0          1m

# oc logs -n lograte-500mpsec-0   centos-logtest-9mgpq | tail -1
2018-01-03 16:17:13,967 - SVTLogger - INFO - centos-logtest-9mgpq : 30000 : ...
```

Compare the result in es:

```sh
### rsh to es pod
sh-4.2$ curl --connect-timeout 2 -s -k --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key https://logging-es:9200/_cat/indices?v | grep logr
```

## Result
One pod for the logging test tool:

```sh
--line-length 1024 --word-length 7 --rate <rate> --time 0 --fixed-line --num-lines <num_lines>
```



| oc                            | logging images | rate (logs/min) | logs generated | logs in es |
|-------------------------------|----------------|----------------:|---------------:|-----------:|
| 3.9.0-0.9.0.git.0.f5a6e1d.el7 | v3.9           |            9000 |          10000 |      10000 |
|                               |                |           12000 |          10000 |       1451 |
|                               |                |           15000 |           8000 |       8000 |
|                               |                |           15000 |          10000 |       1451 |
|                               |                |           30000 |           8000 |       8000 |
|                               |                |           30000 |          10000 |       1451 |
|                               |                |            9000 |         500000 |            |